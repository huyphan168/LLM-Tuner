## Meta Configs for Essai
name: gpt-jt-6b
description: gpt-j model with instructive prompts
model_argument: NI
dataset_argument: NI
training_argument: NI
## Huggingface Configs
run_name: gpt-jt-experiment
deepspeed: essai/ds_configs/stage3.config
do_train: True
do_predict: True
do_eval: True
predict_with_generate: True
model_name_or_path: "togethercomputer/GPT-JT-6B-v1"
max_source_length: 512
max_target_length: 512
generation_max_length: 512
max_num_instances_per_task: 100
max_num_instances_per_eval_task: 100
data_file: "essai/dataset/natural_instructions/ni_dataset.py"
add_task_name: False
add_task_definition: True
num_pos_examples: 2
num_neg_examples: 0
add_explanation: False
tk_instruct: False
data_dir: data/splits/default
task_dir: data/tasks
output_dir: output/gpt-jt-experiment
overwrite_output_dir: True
overwrite_cache: True
per_device_train_batch_size: 1
per_device_eval_batch_size: 2
gradient_accumulation_steps: 4
learning_rate: 5.0e-5
num_train_epochs: 2
lr_scheduler_type: constant
warmup_steps: 0
logging_strategy: steps
logging_steps: 500
evaluation_strategy: "no"
save_strategy: steps
save_steps: 2500
bf16: True
